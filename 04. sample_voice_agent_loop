# This while True: loop -
# Takes microphone input,
# Runs the full ASR ‚Üí LLM ‚Üí TTS pipeline,
# Replies to the user aloud.

# Whisper tiny for speech-to-text (ASR)
# Gemma 3B (quantized) for LLM inference
# Coqui TTS for text-to-speech

#Uses your microphone for input
#Talks back using offline TTS
#Runs fully on-device, no internet needed

pip install openai-whisper torch transformers accelerate bitsandbytes sentencepiece TTS sounddevice scipy

import torch
import whisper
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from TTS.api import TTS
import sounddevice as sd
from scipy.io.wavfile import write
import tempfile
import os

# ========== 1. LOAD MODELS ========== #

print("Loading Whisper tiny...")
asr_model = whisper.load_model("tiny")  # Use "base" if needed

print("Loading quantized Gemma 3B...")
gemma_model_id = "google/gemma-1.1-2b-it"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
)
tokenizer = AutoTokenizer.from_pretrained(gemma_model_id, use_fast=True)
llm_model = AutoModelForCausalLM.from_pretrained(
    gemma_model_id,
    device_map="auto",
    quantization_config=bnb_config
)
llm_model.eval()

print("Loading Coqui TTS...")
tts = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=False, gpu=False)

# ========== 2. BUILD UTILITY FUNCTIONS ========== #

def record_audio(duration=5, samplerate=16000):
    print(f"üéôÔ∏è Recording for {duration} seconds...")
    audio = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1)
    sd.wait()
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        write(f.name, samplerate, audio)
        return f.name

def transcribe(audio_file):
    result = asr_model.transcribe(audio_file)
    return result["text"]

def build_prompt(user_query):
    sys_prompt = (
        "You are a helpful vz assistant that answers customer questions about "
        "device settings, connectivity, troubleshooting, eSIM, and usage tips."
    )
    return f"{sys_prompt}\n\n<start_of_turn>user\n{user_query}<end_of_turn>\n<start_of_turn>model\n"

def generate_response(prompt, max_tokens=256):
    inputs = tokenizer(prompt, return_tensors="pt").to(llm_model.device)
    with torch.no_grad():
        output = llm_model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response.split("<start_of_turn>model")[-1].strip()

def speak(text):
    print("üó£Ô∏è Speaking...")
    tts.tts_to_file(text=text, file_path="response.wav")
    os.system("afplay response.wav" if os.name == "posix" else "start response.wav")  # macOS vs Windows

# ========== 3. RUN AGENT LOOP ========== #

print("ü§ñ Voice Agent Ready. Say 'exit' to stop.\n")

while True:
    # Step 1: Record User
    audio_path = record_audio(duration=5)

    # Step 2: ASR
    user_text = transcribe(audio_path)
    print(f" You said: {user_text}")
    if "exit" in user_text.lower():
        print(" Goodbye!")
        break

    # Step 3: Prompt & LLM
    full_prompt = build_prompt(user_text)
    response_text = generate_response(full_prompt)
    print(f"ü§ñ Agent: {response_text}")

    # Step 4: TTS
    speak(response_text)

    # Cleanup
    os.remove(audio_path)


Output
#You‚Äôll talk into the mic.
#Agent listens, understands, generates a vz-specific reply.
#Then speaks it back to you.

